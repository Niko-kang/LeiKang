<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Lei Kang | 康雷</title>

    <meta name="author" content="Yuxing Long">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>✨</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Lei Kang | 康雷
                </p>
                <p>I am a first-year PhD candidate in <a href="https://cfcs.pku.edu.cn/english/"> Center on Frontiers of Computing Studies (CFCS)</a> at <a href="https://english.pku.edu.cn/">Peking University</a>, advised by Prof. <a href="https://zsdonghao.github.io/">Hao Dong</a>. Before this, I obtained my Bachelor's and Master's degrees from Beijing University of Posts and Telecommunications (BUPT).
                </p>
                <p>My research interests include robot manipulation and embodied navigation.
                </p>
                <p>
                 Email: nikola@stu.pku.edu.cn
                </p>
                <p style="text-align:center">
                  <a href="nikola@stu.pku.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Niko-kang">Github</a> &nbsp;/&nbsp;
		  <a href="https://scholar.google.com/citations?user=DhJp4ZEAAAAJ&hl=zh-CN">Google Scholar</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%;text-align:center;">
                <a href="images/photo.jpg"><img style="width:70%;max-width:70%" alt="profile photo" src="images/photo.jpg" class="hoverZoomLink"></a>
                <br>
                <table style="width:100%;"><tr><td></td></tr></table>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  

      <tr onmouseout="realappliance_stop()" onmouseover="realappliance_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='realappliance_image'>
            <img src='images/2025RealAppliance.jpg' width="170"></div>
            <img src='images/2025RealAppliance.jpg' width="170">
          </div>
          <script type="text/javascript">
            function realappliance_start() {
              document.getElementById('realappliance_image').style.opacity = "1";
            }

            function realappliance_stop() {
              document.getElementById('realappliance_image').style.opacity = "0";
            }
            realappliance_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="">
            <span class="papertitle">RealAppliance: Let High-fidelity Appliance Assets Controllable and Workable as Aligned Real Manuals</span>
          </a>
          <br>
          Yuzheng Gao*, <strong>Yuxing Long*</strong>, Lei Kang, Yuchong Guo, Ziyan Yu, Shangqing Mao, Jiyao Zhang, Ruihai Wu, Dongjiang Li, Hui Shen, Hao Dong
          <br>
          <em>Arxiv</em>
          <br>
          <a href="https://www.arxiv.org/abs/2512.00287">Paper</a>
	  /
	  <a href="https://realappliance.github.io/">Project</a>
          <p></p>
          <p>
          The first appliance digital assets controllable and workable as aligned real manuals.
          </p>
        </td>
      </tr>

			
      <tr onmouseout="navspace_stop()" onmouseover="navspace_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='navspace_image'>
            <img src='images/2025NavSpace.jpg' width="170"></div>
            <img src='images/2025NavSpace.jpg' width="170">
          </div>
          <script type="text/javascript">
            function navspace_start() {
              document.getElementById('navspace_image').style.opacity = "1";
            }

            function navspace_stop() {
              document.getElementById('navspace_image').style.opacity = "0";
            }
            navspace_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="">
            <span class="papertitle">NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions</span>
          </a>
          <br>
          Haolin Yang*, <strong>Yuxing Long*</strong>, Zhuoyuan Yu, Zihan Yang, Minghan Wang, Jiapeng Xu, Yihan Wang, Ziyan Yu, Wenzhe Cai, Lei Kang, Hao Dong
          <br>
          <em>Arxiv</em>
          <br>
          <a href="https://arxiv.org/abs/2510.08173">Paper</a>
	  /
	  <a href="https://github.com/TidalHarley/NavSpace">Project</a>
          <p></p>
          <p>
          The first benchmark and model for navigation spatial intelligence.
          </p>
        </td>
      </tr>
			  
      <tr onmouseout="correctnav_stop()" onmouseover="correctnav_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='correctnav_image'>
            <img src='images/2025CorrectNav.jpg' width="170"></div>
            <img src='images/2025CorrectNav.jpg' width="170">
          </div>
          <script type="text/javascript">
            function correctnav_start() {
              document.getElementById('correctnav_image').style.opacity = "1";
            }

            function correctnav_stop() {
              document.getElementById('correctnav_image').style.opacity = "0";
            }
            correctnav_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="">
            <span class="papertitle">CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model</span>
          </a>
          <br>
          Zhuoyuan Yu*, <strong>Yuxing Long*</strong>, Zihan Yang, Chengyan Zeng, Hongwei Fan, Jiyao Zhang, Hao Dong
          <br>
          <em>AAAI Conference on Artificial Intelligence 2026</em>
          <br>
          <a href="https://arxiv.org/abs/2508.10416">Paper</a>
	  /
	  <a href="https://correctnav.github.io/">Project</a>
          <p></p>
          <p>
          New SOTA Performance on R2R-CE and RxR-CE
          </p>
        </td>
      </tr>
			  
      <tr onmouseout="checkmanual_stop()" onmouseover="checkmanual_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='checkmanual_image'>
            <img src='images/2025CVPR_CheckManual.jpg' width="170"></div>
            <img src='images/2025CVPR_CheckManual.jpg' width="170">
          </div>
          <script type="text/javascript">
            function checkmanual_start() {
              document.getElementById('checkmanual_image').style.opacity = "1";
            }

            function checkmanual_stop() {
              document.getElementById('checkmanual_image').style.opacity = "0";
            }
            checkmanual_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="">
            <span class="papertitle">CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation</span>
          </a>
          <br>
          <strong>Yuxing Long</strong>, Jiyao Zhang, Mingjie Pan, Tianshu Wu, Taewhan Kim, Hao Dong
          <br>
          <em>Conference on Computer Vision and Pattern Recognition (CVPR) 2025</em>
	  <b style="color: red;">(Highlight)</b>
          <br>
          <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation_CVPR_2025_paper.pdf">Paper</a>
	  /
	  <a href="https://sites.google.com/view/checkmanual">Project</a>
	  /
	  <a href="https://github.com/LYX0501/CheckManual">Code</a>
	  /
	  <a href="https://mp.weixin.qq.com/s/xsHcnIbJkhj1peDDbbAe3w">机器之心</a>
          <p></p>
          <p>
          The first benchmark for manual-based appliance manipulation.
          </p>
        </td>
      </tr>

      <tr onmouseout="manipllm_stop()" onmouseover="manipllm_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='manipllm_image'>
            <img src='images/2024CVPR_ManipLLM.jpg' width="170"></div>
            <img src='images/2024CVPR_ManipLLM.jpg' width="170">
          </div>
          <script type="text/javascript">
            function manipllm_start() {
              document.getElementById('manipllm_image').style.opacity = "1";
            }

            function manipllm_stop() {
              document.getElementById('manipllm_image').style.opacity = "0";
            }
            checkmanual_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2312.16217">
            <span class="papertitle">ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation</span>
          </a>
          <br>
          Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, <strong>Yuxing Long</strong>, Yan Shen, Renrui Zhang, Jiaming Liu, Hao Dong
          <br>
          <em>Conference on Computer Vision and Pattern Recognition (CVPR) 2024</em>
          <br>
          <a href="https://arxiv.org/pdf/2312.16217">Paper</a>
	  /
	  <a href="https://github.com/clorislili/ManipLLM">Code</a>
          <p></p>
        </td>
      </tr>
		  
      <tr onmouseout="instructnav_stop()" onmouseover="instructnav_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='InstructNav_image'>
            <img src='images/2024InstructNav.jpg' width="170"></div>
            <img src='images/2024InstructNav.jpg' width="170">
          </div>
          <script type="text/javascript">
            function instructnav_start() {
              document.getElementById('InstructNav_image').style.opacity = "1";
            }

            function instructnav_stop() {
              document.getElementById('InstructNav_image').style.opacity = "0";
            }
            instructnav_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2406.04882.pdf">
            <span class="papertitle">InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment</span>
          </a>
          <br>
          <strong>Yuxing Long*</strong>, Wenzhe Cai*, Hongcheng Wang, Guanqi Zhan, Hao Dong
          <br>
          <em>Conference on Robot Learning (CoRL) 2024</em>
          <br>
          <a href="https://arxiv.org/pdf/2406.04882.pdf">Paper</a>
	  /
	  <a href="https://sites.google.com/view/instructnav">Project</a>
	  /
	  <a href="https://github.com/LYX0501/InstructNav">Code</a>
	  /
	  <a href="https://mp.weixin.qq.com/s/T145ZQlDBTWyX621mNVYGQ">量子位</a>
          <p></p>
          <p>
          The first zero-shot generic instruction navigation system without any pre-built maps.
          </p>
        </td>
      </tr>
    
      <tr onmouseout="discussnav_stop()" onmouseover="discussnav_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='DiscussNav_image'>
            <img src='images/2023DiscussNav.jpg' width="170"></div>
            <img src='images/2023DiscussNav.jpg' width="170">
          </div>
          <script type="text/javascript">
            function discussnav_start() {
              document.getElementById('DiscussNav_image').style.opacity = "1";
            }

            function discussnav_stop() {
              document.getElementById('DiscussNav_image').style.opacity = "0";
            }
            discussnav_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2309.11382.pdf">
            <span class="papertitle">Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions</span>
          </a>
          <br>
          <strong>Yuxing Long</strong>, Xiaoqi Li, Wenzhe Cai, Hao Dong
          <br>
          <em>International Conference on Robotics and Automation (ICRA) 2024</em>
          <br>
          <a href="https://arxiv.org/pdf/2309.11382.pdf">Paper</a>
	  /
	  <a href="https://sites.google.com/view/discussnav">Project</a>
	  /
	  <a href="https://github.com/LYX0501/DiscussNav">Code</a>
	  /
	  <a href="https://mp.weixin.qq.com/s/fJ99zkIyx_-xQglD-brCgg">量子位</a>
          <p></p>
          <p>
          DiscussNav agent actively discusses with multiple domain experts before moving.
          </p>
        </td>
      </tr>

      <tr onmouseout="pixelnav_stop()" onmouseover="pixelnav_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='PixelNav_image'>
            <img src='images/2023PixNav.png' width="170"></div>
            <img src='images/2023PixNav.png' width="170">
          </div>
          <script type="text/javascript">
            function pixelnav_start() {
              document.getElementById('PixelNav_image').style.opacity = "1";
            }

            function pixelnav_stop() {
              document.getElementById('PixelNav_image').style.opacity = "0";
            }
            pixelnav_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2309.10309">
            <span class="papertitle">Bridging Zero-Shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill</span>
          </a>
          <br>
          Wenzhe Cai, Siyuan Huang, Guangran Cheng, <strong>Yuxing Long</strong>, Peng Gao, Changyin Sun, Hao Dong
          <br>
          <em>International Conference on Robotics and Automation (ICRA) 2024</em>
          <br>
          <a href="https://arxiv.org/pdf/2309.10309">Paper</a>
	  /
	  <a href="https://sites.google.com/view/pixnav/">Project</a>
	  /
	  <a href="https://github.com/wzcai99/Pixel-Navigator">Code</a>
          <p></p>
          <p>
          PixNav, a pure RGB-based navigation skill that uses a specified pixel as the goal and can navigate towards any object.
          </p>
        </td>
      </tr>

      <tr onmouseout="dstc11_stop()" onmouseover="dstc11_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='DSTC11_image'>
            <img src='images/2023DSTC11.jpg' width="170"></div>
            <img src='images/2023DSTC11.jpg' width="170">
          </div>
          <script type="text/javascript">
            function dstc11_start() {
              document.getElementById('DSTC11_image').style.opacity = "1";
            }

            function dstc11_stop() {
              document.getElementById('DSTC11_image').style.opacity = "0";
            }
            dstc11_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://github.com/LYX0501/DAMO-ConvAI/tree/main/dstc11-simmc">
            <span class="papertitle">Improving Situated Conversational Agents with Step-by-Step Multi-modal Logic Reasoning</span>
          </a>
          <br>
          <strong>Yuxing Long</strong>*, Huibin Zhang*, Binyuan Hui*, Zhenglu Yang, Caixia Yuan, Xiaojie Wang, Fei Huang, Yongbin Li
          <br>
          <em><b style="color: red;">Champion of SIMMC 2.1 Competition</b></em>, 
          <em>DSTC 11 Workshop</em>
          <b style="color: red;">(Best Paper)</b>
          <br>
	  <a href="https://aclanthology.org/2023.dstc-1.3.pdf">Paper</a>
          /
          <a href="https://github.com/LYX0501/DAMO-ConvAI/tree/main/dstc11-simmc">Code</a>
          <p></p>
          <p>
          We propose a dual-system framework to conduct multimodal logic reasoning step-by-step.
          </p>
        </td>
      </tr>
            
      <tr onmouseout="reg_stop()" onmouseover="reg_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='REG_image'>
            <img src='images/2023MM_REG.png' width="170"></div>
            <img src='images/2023MM_REG.png' width="170">
          </div>
          <script type="text/javascript">
            function reg_start() {
              document.getElementById('REG_image').style.opacity = "1";
            }

            function reg_stop() {
              document.getElementById('REG_image').style.opacity = "0";
            }
            reg_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2308.09977v1.pdf">
            <span class="papertitle">Whether you can locate or not? Interactive Referring Expression Generation</span>
          </a>
          <br>
          Fulong Ye, <strong>Yuxing Long</strong>, Fangxiang Feng, Xiaojie Wang
          <br>
          <em>ACM Multimedia (MM) 2023</em>
          <br>
          <a href="https://arxiv.org/pdf/2308.09977v1.pdf">Paper</a>
          /
          <a href="https://github.com/superhero-7/ireg">Code</a>
          <p></p>
          <p>
          We generate referring expressions by multi-round communications.
          </p>
        </td>
      </tr>
            
      <tr onmouseout="sure_stop()" onmouseover="sure_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='SURE_image'>
            <img src='images/2023ACL_SURE.jpg' width="170"></div>
            <img src='images/2023ACL_SURE.jpg' width="170">
          </div>
          <script type="text/javascript">
            function sure_start() {
              document.getElementById('SURE_image').style.opacity = "1";
            }

            function sure_stop() {
              document.getElementById('SURE_image').style.opacity = "0";
            }
            sure_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2305.18212.pdf">
            <span class="papertitle">Multimodal Recommendation Dialog with Subjective Preference: A New Challenge and Benchmark</span>
          </a>
          <br>
          <strong>Yuxing Long</strong>, Binyuan Hui, Caixia Yuan, Fei Huang, Yongbin Li, Xiaojie Wang
          <br>
          <em>Findings of the Association for Computational Linguistics (Findings of ACL) 2023</em>
          <br>
          <a href="https://arxiv.org/pdf/2305.18212.pdf">Paper</a>
          <p></p>
          <p>
          A new dataset for multimodal recommendation dialog with subjective preferences.
          </p>
        </td>
      </tr>

      <tr onmouseout="spring_stop()" onmouseover="spring_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='SPRING_image'>
            <img src='images/2023AAAI_SPRING.png' width="170"></div>
            <img src='images/2023AAAI_SPRING.png' width="170">
          </div>
          <script type="text/javascript">
            function spring_start() {
              document.getElementById('SPRING_image').style.opacity = "1";
            }

            function spring_stop() {
              document.getElementById('SPRING_image').style.opacity = "0";
            }
            spring_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2301.01949.pdf">
            <span class="papertitle">SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph</span>
          </a>
          <br>
          <strong>Yuxing Long</strong>, Binyuan Hui, Fulong Ye, Yanyang Li, Zhuoxin Han, Caixia Yuan, Yongbin Li, Xiaojie Wang
          <br>
          <em>AAAI Conference on Artificial Intelligence 2023</em>
          <b style="color: red;">(Oral)</b>
          <br>
          <a href="https://arxiv.org/pdf/2301.01949.pdf">Paper</a>
          /
          <a href="https://github.com/LYX0501/SPRING">Code</a>
          <p></p>
          <p>
          We improve the situated conversation agent through novel multimodal question-answering pretraining tasks.
          </p>
        </td>
      </tr>
            
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Services</h2>
                <br> 
                <p>
                  Reviewer: RAL <em>2025</em>
		  <br>
                  Reviewer: ACM MM <em>2023</em>
		  <br>
		  Reviewer: NeurIPS <em>2023</em> <a href="https://sslneurips23.github.io/">Self-Supervised Learning - Theory and Practice Workshop</a>
		  <br>
		  Reviewer: NeurIPS <em>2022</em> <a href="https://neurips-hill.github.io/">Human in the Loop Learning (HiLL) Workshop</a>
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Selected Awards and Honors</h2>
                <br> 
                <p>
		  Outstanding Master's Thesis of Beijing University of Posts and Telecommunications, 2024 <br>
		  Outstanding Graduate of Beijing, 2024 <br>
		  Outstanding Graduate of Beijing University of Posts and Telecommunications, 2024 <br>
                  National Scholarship, 2023 <br>
		  Excellent Graduate Student of Beijing University of Posts and Telecommunications, 2023 <br>
                </p>
              </td>
            </tr>
          </tbody></table>
		
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
